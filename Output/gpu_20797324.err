Loaded module: cuda/11.6
Map:   0%|          | 0/1009 [00:00<?, ? examples/s]Map:  99%|█████████▉| 1000/1009 [00:00<00:00, 5207.73 examples/s]Map: 100%|██████████| 1009/1009 [00:00<00:00, 4990.92 examples/s]
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 5.4.268, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/426 [00:00<?, ?it/s]  0%|          | 1/426 [00:00<05:19,  1.33it/s]  1%|          | 5/426 [00:00<00:59,  7.07it/s]  2%|▏         | 8/426 [00:01<00:38, 10.76it/s]  3%|▎         | 11/426 [00:01<00:29, 13.93it/s]  3%|▎         | 14/426 [00:01<00:24, 16.82it/s]  4%|▍         | 17/426 [00:01<00:20, 19.51it/s]  5%|▍         | 20/426 [00:01<00:18, 21.96it/s]  5%|▌         | 23/426 [00:01<00:17, 23.05it/s]  6%|▋         | 27/426 [00:01<00:15, 25.45it/s]  7%|▋         | 31/426 [00:01<00:14, 26.86it/s]  8%|▊         | 34/426 [00:01<00:14, 27.28it/s]  9%|▊         | 37/426 [00:02<00:14, 27.37it/s] 10%|▉         | 41/426 [00:02<00:13, 28.40it/s] 10%|█         | 44/426 [00:02<00:13, 28.72it/s] 11%|█         | 47/426 [00:02<00:13, 29.03it/s] 12%|█▏        | 51/426 [00:02<00:12, 30.21it/s] 13%|█▎        | 55/426 [00:02<00:12, 30.16it/s] 14%|█▍        | 59/426 [00:02<00:12, 30.23it/s] 15%|█▍        | 63/426 [00:02<00:12, 29.44it/s] 15%|█▌        | 66/426 [00:03<00:12, 29.36it/s] 16%|█▌        | 69/426 [00:03<00:12, 29.48it/s] 17%|█▋        | 72/426 [00:03<00:12, 29.25it/s] 18%|█▊        | 76/426 [00:03<00:11, 29.30it/s] 19%|█▉        | 80/426 [00:03<00:11, 30.01it/s] 20%|█▉        | 84/426 [00:03<00:11, 30.55it/s] 21%|██        | 88/426 [00:03<00:11, 30.30it/s] 22%|██▏       | 92/426 [00:03<00:10, 30.67it/s] 23%|██▎       | 96/426 [00:04<00:10, 30.49it/s] 23%|██▎       | 100/426 [00:04<00:10, 30.65it/s] 24%|██▍       | 104/426 [00:04<00:10, 30.40it/s] 25%|██▌       | 108/426 [00:04<00:10, 30.50it/s] 26%|██▋       | 112/426 [00:04<00:10, 29.94it/s] 27%|██▋       | 116/426 [00:04<00:10, 30.18it/s] 28%|██▊       | 120/426 [00:04<00:10, 30.06it/s] 29%|██▉       | 124/426 [00:04<00:10, 30.02it/s] 30%|███       | 128/426 [00:05<00:09, 30.32it/s] 31%|███       | 132/426 [00:05<00:09, 29.74it/s] 32%|███▏      | 135/426 [00:05<00:09, 29.55it/s] 33%|███▎      | 139/426 [00:05<00:09, 29.59it/s] 34%|███▎      | 143/426 [00:05<00:09, 30.03it/s] 35%|███▍      | 147/426 [00:05<00:09, 30.07it/s] 35%|███▌      | 151/426 [00:05<00:09, 29.93it/s] 36%|███▋      | 155/426 [00:05<00:08, 30.49it/s] 37%|███▋      | 159/426 [00:06<00:08, 29.99it/s] 38%|███▊      | 163/426 [00:06<00:08, 29.42it/s] 39%|███▉      | 166/426 [00:06<00:09, 28.62it/s] 40%|███▉      | 170/426 [00:06<00:08, 29.40it/s] 41%|████      | 173/426 [00:06<00:08, 29.21it/s] 41%|████▏     | 176/426 [00:06<00:08, 29.29it/s] 42%|████▏     | 179/426 [00:06<00:08, 29.17it/s] 43%|████▎     | 182/426 [00:06<00:08, 29.22it/s] 44%|████▎     | 186/426 [00:07<00:08, 29.51it/s] 44%|████▍     | 189/426 [00:07<00:08, 29.55it/s] 45%|████▌     | 193/426 [00:07<00:07, 30.51it/s] 46%|████▌     | 197/426 [00:07<00:07, 29.20it/s] 47%|████▋     | 200/426 [00:07<00:07, 29.18it/s] 48%|████▊     | 204/426 [00:07<00:07, 29.89it/s] 49%|████▊     | 207/426 [00:07<00:07, 29.67it/s] 50%|████▉     | 211/426 [00:07<00:07, 29.96it/s]
  0%|          | 0/81 [00:00<?, ?it/s][A
 11%|█         | 9/81 [00:00<00:00, 89.42it/s][A
 22%|██▏       | 18/81 [00:00<00:00, 75.87it/s][A
 32%|███▏      | 26/81 [00:00<00:00, 67.61it/s][A
 41%|████      | 33/81 [00:00<00:00, 66.93it/s][A
 49%|████▉     | 40/81 [00:00<00:00, 67.03it/s][A
 59%|█████▉    | 48/81 [00:00<00:00, 70.53it/s][A
 72%|███████▏  | 58/81 [00:00<00:00, 77.70it/s][A
 84%|████████▍ | 68/81 [00:00<00:00, 83.38it/s][A
 96%|█████████▋| 78/81 [00:00<00:00, 87.94it/s][A/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                 
                                               [A 50%|█████     | 213/426 [00:09<00:07, 29.96it/s]
100%|██████████| 81/81 [00:01<00:00, 87.94it/s][A
                                               [A 50%|█████     | 214/426 [00:10<00:59,  3.56it/s] 51%|█████     | 217/426 [00:10<00:44,  4.68it/s] 52%|█████▏    | 220/426 [00:11<00:33,  6.10it/s] 53%|█████▎    | 224/426 [00:11<00:23,  8.44it/s] 53%|█████▎    | 227/426 [00:11<00:19, 10.47it/s] 54%|█████▍    | 231/426 [00:11<00:14, 13.42it/s] 55%|█████▍    | 234/426 [00:11<00:12, 15.57it/s] 56%|█████▌    | 238/426 [00:11<00:09, 18.83it/s] 57%|█████▋    | 242/426 [00:11<00:08, 21.65it/s] 58%|█████▊    | 245/426 [00:11<00:07, 22.90it/s] 58%|█████▊    | 248/426 [00:11<00:07, 23.83it/s] 59%|█████▉    | 251/426 [00:12<00:06, 25.22it/s] 60%|█████▉    | 254/426 [00:12<00:06, 26.08it/s] 61%|██████    | 258/426 [00:12<00:06, 27.58it/s] 61%|██████▏   | 261/426 [00:12<00:05, 27.88it/s] 62%|██████▏   | 264/426 [00:12<00:05, 28.32it/s] 63%|██████▎   | 268/426 [00:12<00:05, 29.08it/s] 64%|██████▍   | 272/426 [00:12<00:05, 29.68it/s] 65%|██████▍   | 276/426 [00:12<00:04, 30.35it/s] 66%|██████▌   | 280/426 [00:13<00:04, 29.67it/s] 67%|██████▋   | 284/426 [00:13<00:04, 30.23it/s] 68%|██████▊   | 288/426 [00:13<00:04, 30.25it/s] 69%|██████▊   | 292/426 [00:13<00:04, 30.10it/s] 69%|██████▉   | 296/426 [00:13<00:04, 30.33it/s] 70%|███████   | 300/426 [00:13<00:04, 30.07it/s] 71%|███████▏  | 304/426 [00:13<00:04, 29.22it/s] 72%|███████▏  | 307/426 [00:13<00:04, 28.60it/s] 73%|███████▎  | 311/426 [00:14<00:03, 28.94it/s] 74%|███████▎  | 314/426 [00:14<00:03, 28.96it/s] 74%|███████▍  | 317/426 [00:14<00:03, 29.04it/s] 75%|███████▌  | 321/426 [00:14<00:03, 29.88it/s] 76%|███████▌  | 324/426 [00:14<00:03, 29.71it/s] 77%|███████▋  | 328/426 [00:14<00:03, 29.70it/s] 78%|███████▊  | 331/426 [00:14<00:03, 29.40it/s] 78%|███████▊  | 334/426 [00:14<00:03, 29.35it/s] 79%|███████▉  | 337/426 [00:14<00:03, 29.25it/s] 80%|███████▉  | 340/426 [00:15<00:02, 29.44it/s] 81%|████████  | 344/426 [00:15<00:02, 29.95it/s] 81%|████████▏ | 347/426 [00:15<00:02, 29.91it/s] 82%|████████▏ | 351/426 [00:15<00:02, 30.04it/s] 83%|████████▎ | 354/426 [00:15<00:02, 29.77it/s] 84%|████████▍ | 358/426 [00:15<00:02, 30.18it/s] 85%|████████▍ | 362/426 [00:15<00:02, 30.33it/s] 86%|████████▌ | 366/426 [00:15<00:02, 29.89it/s] 87%|████████▋ | 369/426 [00:16<00:01, 29.67it/s] 87%|████████▋ | 372/426 [00:16<00:01, 29.72it/s] 88%|████████▊ | 375/426 [00:16<00:01, 29.77it/s] 89%|████████▊ | 378/426 [00:16<00:01, 29.56it/s] 89%|████████▉ | 381/426 [00:16<00:01, 29.21it/s] 90%|█████████ | 384/426 [00:16<00:01, 29.28it/s] 91%|█████████ | 388/426 [00:16<00:01, 29.30it/s] 92%|█████████▏| 392/426 [00:16<00:01, 29.70it/s] 93%|█████████▎| 395/426 [00:16<00:01, 29.63it/s] 93%|█████████▎| 398/426 [00:17<00:00, 29.56it/s] 94%|█████████▍| 402/426 [00:17<00:00, 29.77it/s] 95%|█████████▌| 406/426 [00:17<00:00, 29.86it/s] 96%|█████████▌| 409/426 [00:17<00:00, 29.84it/s] 97%|█████████▋| 412/426 [00:17<00:00, 29.84it/s] 98%|█████████▊| 416/426 [00:17<00:00, 29.75it/s] 98%|█████████▊| 419/426 [00:17<00:00, 29.35it/s] 99%|█████████▉| 422/426 [00:17<00:00, 29.36it/s]100%|█████████▉| 425/426 [00:17<00:00, 29.06it/s]
  0%|          | 0/81 [00:00<?, ?it/s][A
 12%|█▏        | 10/81 [00:00<00:00, 91.82it/s][A
 25%|██▍       | 20/81 [00:00<00:00, 80.26it/s][A
 36%|███▌      | 29/81 [00:00<00:00, 70.18it/s][A
 46%|████▌     | 37/81 [00:00<00:00, 72.17it/s][A
 56%|█████▌    | 45/81 [00:00<00:00, 70.45it/s][A
 68%|██████▊   | 55/81 [00:00<00:00, 78.13it/s][A
 80%|████████  | 65/81 [00:00<00:00, 84.56it/s][A
 94%|█████████▍| 76/81 [00:00<00:00, 90.64it/s][A                                                 
                                               [A100%|██████████| 426/426 [00:19<00:00, 29.06it/s]
100%|██████████| 81/81 [00:01<00:00, 90.64it/s][A
                                               [A                                                 100%|██████████| 426/426 [00:24<00:00, 29.06it/s]100%|██████████| 426/426 [00:24<00:00, 17.69it/s]
Traceback (most recent call last):
  File "/work3/s174159/ET_LLM_RAG/Dev/EL.py", line 42, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_dir)
  File "/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py", line 855, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2086, in from_pretrained
    return cls._from_pretrained(
  File "/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2325, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py", line 120, in __init__
    raise ValueError(
ValueError: Couldn't instantiate the backend tokenizer from one of: 
(1) a `tokenizers` library serialization file, 
(2) a slow tokenizer instance to convert or 
(3) an equivalent slow tokenizer class to instantiate and convert. 
You need to have sentencepiece installed to convert a slow tokenizer to a fast one.
