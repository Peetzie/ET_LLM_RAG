Loaded module: cuda/11.6
Map:   0%|          | 0/1009 [00:00<?, ? examples/s]Map:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1000/1009 [00:00<00:00, 5207.73 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1009/1009 [00:00<00:00, 4990.92 examples/s]
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 5.4.268, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/426 [00:00<?, ?it/s]  0%|          | 1/426 [00:00<05:19,  1.33it/s]  1%|          | 5/426 [00:00<00:59,  7.07it/s]  2%|â–         | 8/426 [00:01<00:38, 10.76it/s]  3%|â–Ž         | 11/426 [00:01<00:29, 13.93it/s]  3%|â–Ž         | 14/426 [00:01<00:24, 16.82it/s]  4%|â–         | 17/426 [00:01<00:20, 19.51it/s]  5%|â–         | 20/426 [00:01<00:18, 21.96it/s]  5%|â–Œ         | 23/426 [00:01<00:17, 23.05it/s]  6%|â–‹         | 27/426 [00:01<00:15, 25.45it/s]  7%|â–‹         | 31/426 [00:01<00:14, 26.86it/s]  8%|â–Š         | 34/426 [00:01<00:14, 27.28it/s]  9%|â–Š         | 37/426 [00:02<00:14, 27.37it/s] 10%|â–‰         | 41/426 [00:02<00:13, 28.40it/s] 10%|â–ˆ         | 44/426 [00:02<00:13, 28.72it/s] 11%|â–ˆ         | 47/426 [00:02<00:13, 29.03it/s] 12%|â–ˆâ–        | 51/426 [00:02<00:12, 30.21it/s] 13%|â–ˆâ–Ž        | 55/426 [00:02<00:12, 30.16it/s] 14%|â–ˆâ–        | 59/426 [00:02<00:12, 30.23it/s] 15%|â–ˆâ–        | 63/426 [00:02<00:12, 29.44it/s] 15%|â–ˆâ–Œ        | 66/426 [00:03<00:12, 29.36it/s] 16%|â–ˆâ–Œ        | 69/426 [00:03<00:12, 29.48it/s] 17%|â–ˆâ–‹        | 72/426 [00:03<00:12, 29.25it/s] 18%|â–ˆâ–Š        | 76/426 [00:03<00:11, 29.30it/s] 19%|â–ˆâ–‰        | 80/426 [00:03<00:11, 30.01it/s] 20%|â–ˆâ–‰        | 84/426 [00:03<00:11, 30.55it/s] 21%|â–ˆâ–ˆ        | 88/426 [00:03<00:11, 30.30it/s] 22%|â–ˆâ–ˆâ–       | 92/426 [00:03<00:10, 30.67it/s] 23%|â–ˆâ–ˆâ–Ž       | 96/426 [00:04<00:10, 30.49it/s] 23%|â–ˆâ–ˆâ–Ž       | 100/426 [00:04<00:10, 30.65it/s] 24%|â–ˆâ–ˆâ–       | 104/426 [00:04<00:10, 30.40it/s] 25%|â–ˆâ–ˆâ–Œ       | 108/426 [00:04<00:10, 30.50it/s] 26%|â–ˆâ–ˆâ–‹       | 112/426 [00:04<00:10, 29.94it/s] 27%|â–ˆâ–ˆâ–‹       | 116/426 [00:04<00:10, 30.18it/s] 28%|â–ˆâ–ˆâ–Š       | 120/426 [00:04<00:10, 30.06it/s] 29%|â–ˆâ–ˆâ–‰       | 124/426 [00:04<00:10, 30.02it/s] 30%|â–ˆâ–ˆâ–ˆ       | 128/426 [00:05<00:09, 30.32it/s] 31%|â–ˆâ–ˆâ–ˆ       | 132/426 [00:05<00:09, 29.74it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 135/426 [00:05<00:09, 29.55it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 139/426 [00:05<00:09, 29.59it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/426 [00:05<00:09, 30.03it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 147/426 [00:05<00:09, 30.07it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 151/426 [00:05<00:09, 29.93it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 155/426 [00:05<00:08, 30.49it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 159/426 [00:06<00:08, 29.99it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 163/426 [00:06<00:08, 29.42it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 166/426 [00:06<00:09, 28.62it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 170/426 [00:06<00:08, 29.40it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 173/426 [00:06<00:08, 29.21it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 176/426 [00:06<00:08, 29.29it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 179/426 [00:06<00:08, 29.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 182/426 [00:06<00:08, 29.22it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 186/426 [00:07<00:08, 29.51it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 189/426 [00:07<00:08, 29.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 193/426 [00:07<00:07, 30.51it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 197/426 [00:07<00:07, 29.20it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 200/426 [00:07<00:07, 29.18it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 204/426 [00:07<00:07, 29.89it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 207/426 [00:07<00:07, 29.67it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 211/426 [00:07<00:07, 29.96it/s]
  0%|          | 0/81 [00:00<?, ?it/s][A
 11%|â–ˆ         | 9/81 [00:00<00:00, 89.42it/s][A
 22%|â–ˆâ–ˆâ–       | 18/81 [00:00<00:00, 75.87it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 26/81 [00:00<00:00, 67.61it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 33/81 [00:00<00:00, 66.93it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 40/81 [00:00<00:00, 67.03it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 48/81 [00:00<00:00, 70.53it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 58/81 [00:00<00:00, 77.70it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 68/81 [00:00<00:00, 83.38it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 78/81 [00:00<00:00, 87.94it/s][A/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
                                                 
                                               [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 213/426 [00:09<00:07, 29.96it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:01<00:00, 87.94it/s][A
                                               [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 214/426 [00:10<00:59,  3.56it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 217/426 [00:10<00:44,  4.68it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 220/426 [00:11<00:33,  6.10it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 224/426 [00:11<00:23,  8.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 227/426 [00:11<00:19, 10.47it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 231/426 [00:11<00:14, 13.42it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 234/426 [00:11<00:12, 15.57it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 238/426 [00:11<00:09, 18.83it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 242/426 [00:11<00:08, 21.65it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 245/426 [00:11<00:07, 22.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 248/426 [00:11<00:07, 23.83it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 251/426 [00:12<00:06, 25.22it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 254/426 [00:12<00:06, 26.08it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 258/426 [00:12<00:06, 27.58it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 261/426 [00:12<00:05, 27.88it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 264/426 [00:12<00:05, 28.32it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 268/426 [00:12<00:05, 29.08it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 272/426 [00:12<00:05, 29.68it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 276/426 [00:12<00:04, 30.35it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 280/426 [00:13<00:04, 29.67it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 284/426 [00:13<00:04, 30.23it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 288/426 [00:13<00:04, 30.25it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 292/426 [00:13<00:04, 30.10it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 296/426 [00:13<00:04, 30.33it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 300/426 [00:13<00:04, 30.07it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 304/426 [00:13<00:04, 29.22it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 307/426 [00:13<00:04, 28.60it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 311/426 [00:14<00:03, 28.94it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 314/426 [00:14<00:03, 28.96it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 317/426 [00:14<00:03, 29.04it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 321/426 [00:14<00:03, 29.88it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 324/426 [00:14<00:03, 29.71it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 328/426 [00:14<00:03, 29.70it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 331/426 [00:14<00:03, 29.40it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 334/426 [00:14<00:03, 29.35it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 337/426 [00:14<00:03, 29.25it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 340/426 [00:15<00:02, 29.44it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 344/426 [00:15<00:02, 29.95it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 347/426 [00:15<00:02, 29.91it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 351/426 [00:15<00:02, 30.04it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 354/426 [00:15<00:02, 29.77it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/426 [00:15<00:02, 30.18it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 362/426 [00:15<00:02, 30.33it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 366/426 [00:15<00:02, 29.89it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 369/426 [00:16<00:01, 29.67it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 372/426 [00:16<00:01, 29.72it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 375/426 [00:16<00:01, 29.77it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 378/426 [00:16<00:01, 29.56it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 381/426 [00:16<00:01, 29.21it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 384/426 [00:16<00:01, 29.28it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 388/426 [00:16<00:01, 29.30it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 392/426 [00:16<00:01, 29.70it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 395/426 [00:16<00:01, 29.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 398/426 [00:17<00:00, 29.56it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 402/426 [00:17<00:00, 29.77it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 406/426 [00:17<00:00, 29.86it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 409/426 [00:17<00:00, 29.84it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 412/426 [00:17<00:00, 29.84it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 416/426 [00:17<00:00, 29.75it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 419/426 [00:17<00:00, 29.35it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 422/426 [00:17<00:00, 29.36it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 425/426 [00:17<00:00, 29.06it/s]
  0%|          | 0/81 [00:00<?, ?it/s][A
 12%|â–ˆâ–        | 10/81 [00:00<00:00, 91.82it/s][A
 25%|â–ˆâ–ˆâ–       | 20/81 [00:00<00:00, 80.26it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 29/81 [00:00<00:00, 70.18it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 37/81 [00:00<00:00, 72.17it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 45/81 [00:00<00:00, 70.45it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 55/81 [00:00<00:00, 78.13it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 65/81 [00:00<00:00, 84.56it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 76/81 [00:00<00:00, 90.64it/s][A                                                 
                                               [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 426/426 [00:19<00:00, 29.06it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81/81 [00:01<00:00, 90.64it/s][A
                                               [A                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 426/426 [00:24<00:00, 29.06it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 426/426 [00:24<00:00, 17.69it/s]
Traceback (most recent call last):
  File "/work3/s174159/ET_LLM_RAG/Dev/EL.py", line 42, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_dir)
  File "/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py", line 855, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2086, in from_pretrained
    return cls._from_pretrained(
  File "/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2325, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 133, in __init__
    super().__init__(
  File "/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py", line 120, in __init__
    raise ValueError(
ValueError: Couldn't instantiate the backend tokenizer from one of: 
(1) a `tokenizers` library serialization file, 
(2) a slow tokenizer instance to convert or 
(3) an equivalent slow tokenizer class to instantiate and convert. 
You need to have sentencepiece installed to convert a slow tokenizer to a fast one.
