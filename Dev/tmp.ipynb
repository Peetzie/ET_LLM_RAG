{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9997329, 'index': 6, 'word': 'Tom', 'start': 20, 'end': 23}, {'entity': 'I-PER', 'score': 0.9996735, 'index': 7, 'word': 'Holland', 'start': 24, 'end': 31}]\n",
      "Persons: ['Tom Holland']\n",
      "Locations: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying Wikidata: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Result Q2023710:\\nLabel: Tom Holland\\nDescription: British actor\\nAliases: Thomas Stanley Holland\\ninstance of: human\\ncountry of citizenship: England, United Kingdom\\noccupation: voice actor, stage actor, film actor, actor, dancer, television actor\\nsex or gender: male\\ndate of birth: 1996-06-01\\nplace of birth: Kingston upon Thames\\neducated at: BRIT School for Performing Arts and Technology, Wimbledon College, Richard Challoner School, Donhead Preparatory School\\nfield of work: acting\\nnotable work: The Impossible, Spider-Man, Avengers: Infinity War, In the Heart of the Sea, Avengers: Endgame, Spies in Disguise, Onward, Uncharted, Captain America: Civil War\\nfather: Dominic Holland\\n\\nResult Q1340923:\\nLabel: Tom Holland\\nDescription: American scriptwriter and director (born 1943)\\nAliases: Thomas Lee Holland, Thomas Holland\\ninstance of: human\\ncountry of citizenship: United States of America\\noccupation: film director, lawyer, screenwriter, film actor, director, actor, film producer\\nsex or gender: male\\ndate of birth: 1943-07-11\\nplace of birth: Poughkeepsie\\neducated at: UCLA School of Law, University of Southern California, Northwestern University, Worcester Academy\\ngenre: horror film, thriller\\nfield of work: film, film direction, acting\\nchild: Josh Holland']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd143bdfac34152b6b5c5a447ba33a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 40.69 MiB is free. Process 8596 has 9.98 GiB memory in use. Process 20423 has 754.00 MiB memory in use. Process 9094 has 528.00 MiB memory in use. Including non-PyTorch memory, this process has 20.46 GiB memory in use. Of the allocated memory 20.08 GiB is allocated by PyTorch, and 79.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m\n\u001b[1;32m    126\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceH4/zephyr-7b-alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m question_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    128\u001b[0m     model_name, token\u001b[38;5;241m=\u001b[39maccess_key, cache_dir\u001b[38;5;241m=\u001b[39mmodel_dir\n\u001b[1;32m    129\u001b[0m )\n\u001b[0;32m--> 130\u001b[0m question_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccess_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Define the text generation pipeline\u001b[39;00m\n\u001b[1;32m    135\u001b[0m text_generation \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    137\u001b[0m     model\u001b[38;5;241m=\u001b[39mquestion_model,\n\u001b[1;32m    138\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mquestion_tokenizer,\n\u001b[1;32m    139\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    140\u001b[0m )\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/modeling_utils.py:2576\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2573\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2575\u001b[0m         )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 40.69 MiB is free. Process 8596 has 9.98 GiB memory in use. Process 20423 has 754.00 MiB memory in use. Process 9094 has 528.00 MiB memory in use. Including non-PyTorch memory, this process has 20.46 GiB memory in use. Of the allocated memory 20.08 GiB is allocated by PyTorch, and 79.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load variables\n",
    "load_dotenv()\n",
    "# change dir root (one above)\n",
    "access_key = os.getenv(\"HUGGING_FACE\")\n",
    "root_dir = os.getcwd()\n",
    "model_dir = Path(root_dir, \"models\")\n",
    "articles_dir = Path(root_dir, \"Articles\")\n",
    "\n",
    "\n",
    "# Assuming you have the `access_key` for authentication with Hugging Face API\n",
    "\n",
    "## NER\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"dslim/bert-base-NER\", cache_dir=model_dir\n",
    ")\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"dslim/bert-base-NER\", cache_dir=model_dir\n",
    ").to(\"cuda\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer)\n",
    "example = \"Give me a resume of Tom Holland's acting career\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n",
    "\n",
    "\n",
    "def extract_entities(entities):\n",
    "    \"\"\"\n",
    "    Extracts persons and locations from a list of entities recognized by an entity recognition system.\n",
    "\n",
    "    Parameters:\n",
    "    - entities (list of dicts): The output from an entity recognition system, where each dictionary\n",
    "      contains details about the recognized entity, including its type (person or location).\n",
    "\n",
    "    Returns:\n",
    "    - tuple of two lists: (persons, locations) where each is a list of extracted entity names.\n",
    "    \"\"\"\n",
    "    # Initialize lists to hold persons and locations\n",
    "    persons = []\n",
    "    locations = []\n",
    "\n",
    "    # Temporary variables to construct full names and locations\n",
    "    current_person = \"\"\n",
    "    current_location = \"\"\n",
    "\n",
    "    # Iterate over each entity to extract and construct full names and locations\n",
    "    for entity in entities:\n",
    "        if entity[\"entity\"].startswith(\"B-PER\"):\n",
    "            # If there's a current person, append it to persons before starting a new one\n",
    "            if current_person:\n",
    "                persons.append(current_person.strip())\n",
    "                current_person = \"\"\n",
    "            current_person += entity[\"word\"].lstrip(\"##\")\n",
    "        elif entity[\"entity\"].startswith(\"I-PER\") and current_person:\n",
    "            # Handle token splitting for names correctly\n",
    "            if entity[\"word\"].startswith(\"##\"):\n",
    "                current_person += entity[\"word\"].replace(\"##\", \"\")\n",
    "            else:\n",
    "                current_person += \" \" + entity[\"word\"]\n",
    "        elif entity[\"entity\"].startswith(\"B-ORG\"):\n",
    "            # Similarly, for locations\n",
    "            if current_location:\n",
    "                locations.append(current_location.strip())\n",
    "                current_location = \"\"\n",
    "            current_location += entity[\"word\"].lstrip(\"##\")\n",
    "        elif entity[\"entity\"].startswith(\"I-ORG\") and current_location:\n",
    "            if entity[\"word\"].startswith(\"##\"):\n",
    "                current_location += entity[\"word\"].replace(\"##\", \"\")\n",
    "            else:\n",
    "                current_location += \" \" + entity[\"word\"]\n",
    "\n",
    "    # Append any remaining entities to their respective lists\n",
    "    if current_person:\n",
    "        persons.append(current_person.strip())\n",
    "    if current_location:\n",
    "        locations.append(current_location.strip())\n",
    "    words_to_remove = [\"like\", \"the\", \"and\", \"or\", \"but\", \"so\", \"for\", \"in\", \"at\", \"on\"]\n",
    "    cleaned_locations = []\n",
    "    for location in locations:\n",
    "        cleaned_location = \" \".join(\n",
    "            [word for word in location.split() if word.lower() not in words_to_remove]\n",
    "        )\n",
    "        cleaned_locations.append(cleaned_location)\n",
    "    return persons, cleaned_locations\n",
    "\n",
    "\n",
    "persons, locations = extract_entities(ner_results)\n",
    "\n",
    "print(f\"Persons: {persons}\")\n",
    "print(f\"Locations: {locations}\")\n",
    "\n",
    "\n",
    "## Get context\n",
    "\n",
    "from langchain_community.tools.wikidata.tool import WikidataAPIWrapper, WikidataQueryRun\n",
    "\n",
    "wikidata = WikidataQueryRun(api_wrapper=WikidataAPIWrapper())\n",
    "\n",
    "\n",
    "results = []\n",
    "# Iterate through each person and location\n",
    "for item in tqdm(persons + locations, desc=f\"Querying Wikidata\"):\n",
    "    # Run the command and append the output to the results list\n",
    "    result = wikidata.run(item)\n",
    "    results.append(result)\n",
    "print(results)\n",
    "# Load your chosen model\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
    "question_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, token=access_key, cache_dir=model_dir\n",
    ")\n",
    "question_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, token=access_key, cache_dir=model_dir\n",
    ").to(device)\n",
    "\n",
    "# Define the text generation pipeline\n",
    "text_generation = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=question_model,\n",
    "    tokenizer=question_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "# Define your prompt with the context\n",
    "context = str(results)\n",
    "query = example\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an AI Assistant that follows instructions extremely well.\n",
    "Please be truthful and give direct answers\n",
    "\n",
    "{context}\n",
    "\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "response = text_generation(prompt, max_length=1024, temperature=0.7)[0][\n",
    "    \"generated_text\"\n",
    "]\n",
    "\n",
    "# Print or process the response as needed\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
