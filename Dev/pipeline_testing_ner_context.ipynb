{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import wikipediaapi\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "from pathlib import Path\n",
    "\n",
    "# Load variables\n",
    "load_dotenv()\n",
    "# change dir root (one above)\n",
    "access_key = os.getenv(\"HUGGING_FACE\")\n",
    "root_dir = os.getcwd()\n",
    "model_dir = Path(root_dir, \"models\")\n",
    "articles_dir = Path(root_dir, \"Articles\")\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ner_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdslim/bert-base-NER\u001b[39m\u001b[38;5;124m\"\u001b[39m, cache_dir \u001b[38;5;241m=\u001b[39m model_dir)\n\u001b[0;32m----> 2\u001b[0m ner_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdslim/bert-base-NER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mner_model, tokenizer\u001b[38;5;241m=\u001b[39mner_tokenizer)\n\u001b[1;32m      5\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGive me a resume of Tom Holland\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms acting career\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/transformers/modeling_utils.py:2576\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2573\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2575\u001b[0m         )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/work3/s174159/ET_LLM_RAG/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "ner_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\", cache_dir = model_dir)\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\", cache_dir = model_dir).to(\"cuda\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer)\n",
    "example = \"Give me a resume of Tom Holland's acting career\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(entities):\n",
    "    \"\"\"\n",
    "    Extracts persons and locations from a list of entities recognized by an entity recognition system.\n",
    "\n",
    "    Parameters:\n",
    "    - entities (list of dicts): The output from an entity recognition system, where each dictionary\n",
    "      contains details about the recognized entity, including its type (person or location).\n",
    "\n",
    "    Returns:\n",
    "    - tuple of two lists: (persons, locations) where each is a list of extracted entity names.\n",
    "    \"\"\"\n",
    "    # Initialize lists to hold persons and locations\n",
    "    persons = []\n",
    "    locations = []\n",
    "\n",
    "    # Temporary variables to construct full names and locations\n",
    "    current_person = \"\"\n",
    "    current_location = \"\"\n",
    "\n",
    "    # Iterate over each entity to extract and construct full names and locations\n",
    "    for entity in entities:\n",
    "        if entity['entity'].startswith('B-PER'):\n",
    "            # If there's a current person, append it to persons before starting a new one\n",
    "            if current_person:\n",
    "                persons.append(current_person.strip())\n",
    "                current_person = \"\"\n",
    "            current_person += entity['word'].lstrip('##')\n",
    "        elif entity['entity'].startswith('I-PER') and current_person:\n",
    "            # Handle token splitting for names correctly\n",
    "            if entity['word'].startswith('##'):\n",
    "                current_person += entity['word'].replace(\"##\", \"\")\n",
    "            else:\n",
    "                current_person += \" \" + entity['word']\n",
    "        elif entity['entity'].startswith('B-ORG'):\n",
    "            # Similarly, for locations\n",
    "            if current_location:\n",
    "                locations.append(current_location.strip())\n",
    "                current_location = \"\"\n",
    "            current_location += entity['word'].lstrip('##')\n",
    "        elif entity['entity'].startswith('I-ORG') and current_location:\n",
    "            if entity['word'].startswith('##'):\n",
    "                current_location += entity['word'].replace(\"##\", \"\")\n",
    "            else:\n",
    "                current_location += \" \" + entity['word']\n",
    "\n",
    "    # Append any remaining entities to their respective lists\n",
    "    if current_person:\n",
    "        persons.append(current_person.strip())\n",
    "    if current_location:\n",
    "        locations.append(current_location.strip())\n",
    "    words_to_remove = ['like', 'the', 'and', 'or', 'but', 'so', 'for', 'in', 'at', 'on']\n",
    "    cleaned_locations = []\n",
    "    for location in locations:\n",
    "        cleaned_location = ' '.join([word for word in location.split() if word.lower() not in words_to_remove])\n",
    "        cleaned_locations.append(cleaned_location)\n",
    "    return persons, cleaned_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persons: ['Tom Holland']\n",
      "Locations: []\n"
     ]
    }
   ],
   "source": [
    "persons, locations = extract_entities(ner_results)\n",
    "\n",
    "print(f\"Persons: {persons}\")\n",
    "print(f\"Locations: {locations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.wikidata.tool import WikidataAPIWrapper, WikidataQueryRun\n",
    "\n",
    "wikidata = WikidataQueryRun(api_wrapper=WikidataAPIWrapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying Wikidata: 100%|██████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.10s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# Iterate through each person and location\n",
    "for item in tqdm(persons + locations, desc=f\"Querying Wikidata\"):\n",
    "    # Run the command and append the output to the results list\n",
    "    result = wikidata.run(item)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Result Q2023710:\\nLabel: Tom Holland\\nDescription: British actor\\nAliases: Thomas Stanley Holland\\ninstance of: human\\ncountry of citizenship: England, United Kingdom\\noccupation: voice actor, stage actor, film actor, actor, dancer, television actor\\nsex or gender: male\\ndate of birth: 1996-06-01\\nplace of birth: Kingston upon Thames\\neducated at: BRIT School for Performing Arts and Technology, Wimbledon College, Richard Challoner School, Donhead Preparatory School\\nfield of work: acting\\nnotable work: The Impossible, Spider-Man, Avengers: Infinity War, In the Heart of the Sea, Avengers: Endgame, Spies in Disguise, Onward, Uncharted, Captain America: Civil War\\nfather: Dominic Holland\\n\\nResult Q1340923:\\nLabel: Tom Holland\\nDescription: American scriptwriter and director (born 1943)\\nAliases: Thomas Lee Holland, Thomas Holland\\ninstance of: human\\ncountry of citizenship: United States of America\\noccupation: film director, lawyer, screenwriter, film actor, director, actor, film producer\\nsex or gender: male\\ndate of birth: 1943-07-11\\nplace of birth: Poughkeepsie\\neducated at: UCLA School of Law, University of Southern California, Northwestern University, Worcester Academy\\ngenre: horror film, thriller\\nfield of work: film, film direction, acting\\nchild: Josh Holland']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q2023710', 'Q1340923']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qids = []\n",
    "\n",
    "# Iterate through each result\n",
    "for result in results:\n",
    "    # Extract the QIDs from the result\n",
    "    qids += re.findall(r'Q\\d+', result)\n",
    "qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Wikipedia page title from Wikidata QID\n",
    "def get_wikipedia_title(qid, user_agent):\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={qid}&props=sitelinks&format=json\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    data = response.json()\n",
    "    if \"sitelinks\" in data.get(\"entities\", {}).get(qid, {}):\n",
    "        sitelinks = data[\"entities\"][qid][\"sitelinks\"]\n",
    "        if \"enwiki\" in sitelinks:\n",
    "            return sitelinks[\"enwiki\"][\"title\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to fetch Wikipedia article content based on page title\n",
    "def fetch_wikipedia_article(title, num_words, user_agent):\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(\"en\", headers=headers)\n",
    "    page = wiki_wiki.page(title)\n",
    "    if page.exists():\n",
    "        article_content = page.text\n",
    "        # Extract specified number of words\n",
    "        words = article_content.split()[:num_words]\n",
    "        return \" \".join(words)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to download Wikipedia articles\n",
    "def download_wikipedia_article(qid, user_agent, num_words_to_save):\n",
    "    title = get_wikipedia_title(qid, user_agent)\n",
    "    if title:\n",
    "        content = fetch_wikipedia_article(title, num_words_to_save, user_agent)\n",
    "        if content:\n",
    "            # Save article content to disk\n",
    "            root_dir = Path(\".\")\n",
    "            directory = root_dir / \"Articles\"\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            filename = directory / f\"{title}.txt\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(content)\n",
    "        else:\n",
    "            print(\"Failed to fetch article content.\")\n",
    "    else:\n",
    "        print(f\"No Wikipedia article found for QID: {qid}\")\n",
    "\n",
    "def download_wikipedia_article_with_progress(user_agent, num_words_to_save):\n",
    "    qids_list = qids\n",
    "    num_articles = len(qids_list)\n",
    "\n",
    "    # Initialize a single progress bar for the overall process\n",
    "    with tqdm(\n",
    "        total=num_articles, desc=\"Downloading Wikipedia Articles\", unit=\" articles\"\n",
    "    ) as pbar_total:\n",
    "        for qid in qids_list:\n",
    "            title = get_wikipedia_title(qid, user_agent)\n",
    "            if title:\n",
    "                content = fetch_wikipedia_article(title, num_words_to_save, user_agent)\n",
    "                if content:\n",
    "                    # Save article content to disk\n",
    "                    filename = Path(root_dir, f\"Articles/{title}.txt\")\n",
    "                    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                        file.write(content)\n",
    "                    pbar_total.update(\n",
    "                        1\n",
    "                    )  # Update the progress bar for each downloaded article\n",
    "                else:\n",
    "                    print(f\"Failed to fetch content for article with QID: {qid}\")\n",
    "            else:\n",
    "                print(f\"No Wikipedia article found for QID: {qid}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wikipedia Articles:   0%|                                                      | 0/2 [00:00<?, ? articles/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Wikipedia Articles: 100%|██████████████████████████████████████████████| 2/2 [00:01<00:00,  1.41 articles/s]\n"
     ]
    }
   ],
   "source": [
    "download_wikipedia_article_with_progress(\"flarsen\", length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text document from file system\n",
    "def load_text_document(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder path\n",
    "folder_path = \"/work3/s174159/ET_LLM_RAG/Articles/\"\n",
    "\n",
    "# Encode articles in the folder\n",
    "articles = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        content = load_text_document(file_path)\n",
    "        articles.append(content)\n",
    "\n",
    "# Split text into chunks for each article\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=length, chunk_overlap=1)\n",
    "chunking = [text_splitter.split_text(article) for article in articles]\n",
    "\n",
    "\n",
    "# Define a class to represent documents\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "\n",
    "# Convert dictionaries into objects with 'page_content' attribute\n",
    "documents = [Document(chunk, metadata=None) for chunks in chunking for chunk in chunks]\n",
    "\n",
    "# Embed chunks using the specified embedding model\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=access_key, model_name=\"BAAI/bge-base-en-v1.5\"\n",
    ")\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# Define retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform retrieval-based question answering task\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m example\n\u001b[0;32m----> 3\u001b[0m docs_rel \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241m.\u001b[39mget_relevant_documents(query)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate response to a given query using augmented knowledge base\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m HuggingFaceHub(\n\u001b[1;32m      7\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceH4/zephyr-7b-alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m512\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_dir},\n\u001b[1;32m      9\u001b[0m     huggingfacehub_api_token\u001b[38;5;241m=\u001b[39maccess_key,\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform retrieval-based question answering task\n",
    "query = example\n",
    "docs_rel = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Generate response to a given query using augmented knowledge base\n",
    "model = HuggingFaceHub(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_new_tokens\": 1024, \"max_length\": 512, \"cache_dir\": model_dir},\n",
    "    huggingfacehub_api_token=access_key,\n",
    ")\n",
    "qa = RetrievalQA.from_chain_type(llm=model, retriever=retriever)\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an AI Assistant that follows instructions extremely well.\n",
    "Please be truthful and give direct answers\n",
    "</s>\n",
    "\n",
    "{query}\n",
    "</s>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Extract context from the provided information\n",
    "context = str(results)\n",
    "response = qa(context + \"\\n\" + prompt)\n",
    "print(response[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
