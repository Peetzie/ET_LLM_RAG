{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load variables\n",
    "load_dotenv()\n",
    "# change dir root (one above)\n",
    "access_key = os.getenv(\"HUGGING_FACE\")\n",
    "root_dir = os.getcwd()\n",
    "model_dir = Path(root_dir, \"models\")\n",
    "articles_dir = Path(root_dir, \"Articles\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\", cache_dir = model_dir)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\", cache_dir = model_dir).to(\"cuda\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"Give me a resume of Tom Holland's acting career\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(entities):\n",
    "    \"\"\"\n",
    "    Extracts persons and locations from a list of entities recognized by an entity recognition system.\n",
    "\n",
    "    Parameters:\n",
    "    - entities (list of dicts): The output from an entity recognition system, where each dictionary\n",
    "      contains details about the recognized entity, including its type (person or location).\n",
    "\n",
    "    Returns:\n",
    "    - tuple of two lists: (persons, locations) where each is a list of extracted entity names.\n",
    "    \"\"\"\n",
    "    # Initialize lists to hold persons and locations\n",
    "    persons = []\n",
    "    locations = []\n",
    "\n",
    "    # Temporary variables to construct full names and locations\n",
    "    current_person = \"\"\n",
    "    current_location = \"\"\n",
    "\n",
    "    # Iterate over each entity to extract and construct full names and locations\n",
    "    for entity in entities:\n",
    "        if entity['entity'].startswith('B-PER'):\n",
    "            # If there's a current person, append it to persons before starting a new one\n",
    "            if current_person:\n",
    "                persons.append(current_person.strip())\n",
    "                current_person = \"\"\n",
    "            current_person += entity['word'].lstrip('##')\n",
    "        elif entity['entity'].startswith('I-PER') and current_person:\n",
    "            # Handle token splitting for names correctly\n",
    "            if entity['word'].startswith('##'):\n",
    "                current_person += entity['word'].replace(\"##\", \"\")\n",
    "            else:\n",
    "                current_person += \" \" + entity['word']\n",
    "        elif entity['entity'].startswith('B-ORG'):\n",
    "            # Similarly, for locations\n",
    "            if current_location:\n",
    "                locations.append(current_location.strip())\n",
    "                current_location = \"\"\n",
    "            current_location += entity['word'].lstrip('##')\n",
    "        elif entity['entity'].startswith('I-ORG') and current_location:\n",
    "            if entity['word'].startswith('##'):\n",
    "                current_location += entity['word'].replace(\"##\", \"\")\n",
    "            else:\n",
    "                current_location += \" \" + entity['word']\n",
    "\n",
    "    # Append any remaining entities to their respective lists\n",
    "    if current_person:\n",
    "        persons.append(current_person.strip())\n",
    "    if current_location:\n",
    "        locations.append(current_location.strip())\n",
    "    words_to_remove = ['like', 'the', 'and', 'or', 'but', 'so', 'for', 'in', 'at', 'on']\n",
    "    cleaned_locations = []\n",
    "    for location in locations:\n",
    "        cleaned_location = ' '.join([word for word in location.split() if word.lower() not in words_to_remove])\n",
    "        cleaned_locations.append(cleaned_location)\n",
    "    return persons, cleaned_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons, locations = extract_entities(ner_results)\n",
    "\n",
    "print(f\"Persons: {persons}\")\n",
    "print(f\"Locations: {locations}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
